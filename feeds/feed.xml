<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title><![CDATA[Psychology Today – Latest]]></title>
        <description><![CDATA[Latest articles from Psychology Today (full content)]]></description>
        <link>https://www.psychologytoday.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Fri, 27 Feb 2026 02:08:30 GMT</lastBuildDate>
        <atom:link href="https://www.psychologytoday.com/us/feed" rel="self" type="application/rss+xml"/>
        <pubDate>Fri, 27 Feb 2026 02:08:30 GMT</pubDate>
        <language><![CDATA[en]]></language>
        <item>
            <title><![CDATA[Braver New World: The AI Architecture of the Inevitable?]]></title>
            <description><![CDATA[<p><img src="https://cdn2.psychologytoday.com/assets/styles/profile_teaser_small/public/field_user_blogger_photo/image3011-removebg-preview.jpg?itok=W86qiZrS" alt="Darren J. Edwards Ph.D." style="max-width:100%;border-radius:6px;"/></p><p><strong style="text-transform:uppercase;font-size:0.85em;color:#666;">Artificial Intelligence</strong></p><h1 style="font-size:1.6em;margin-bottom:4px;">Braver New World: The AI Architecture of the Inevitable?</h1><h2 style="font-size:1.1em;color:#444;font-weight:normal;margin-top:0;">How AI may design our future and quietly reshape human agency and behavior.</h2><p style="font-size:0.9em;color:#666;">Posted February 26, 2026</p><hr style="border:none;border-top:1px solid #ddd;margin:16px 0;"/>
          <div style="background:#f0f4ff;border-left:4px solid #3a5bd9;padding:12px 16px;margin-bottom:20px;border-radius:0 6px 6px 0;">
            <strong style="display:block;margin-bottom:8px;font-size:1em;">Key points</strong>
            <ul style="margin:0;padding-left:20px;">
              <li style="margin-bottom:6px;">AI may begin to predict and anticipate our behaviors in order to be more helpful.</li><li style="margin-bottom:6px;">AI may start anticipating our choices through context before we consciously make them—framed as assistance.</li><li style="margin-bottom:6px;">In trying to be helpful, AI may quietly learn to predict and steer our behavior.</li>
            </ul>
          </div>
                    <div class="insertArea ck-widget">
<div class="image-article-inline-half">
<div class="insert-inner">
<div class="insert-image"><img class="image-article_inline_half_caption" src="https://cdn2.psychologytoday.com/assets/styles/article_inline_half_caption/public/field_blog_entry_images/2026-02/pic1.png.jpg?itok=U-bv4Vwi" width="320" height="322" data-insert-type="image" title="AI-generated image created by the author." alt="AI-generated image created by the author."></div>
<div class="caption"> </div>
<div class="insertArea--origin subtext">Source: AI-generated image created by the author.</div>
</div>
</div>
</div>
<p>What if the greatest threat to human freedom does not arrive through force but through convenience? As AI systems grow more predictive, anticipating our thoughts, smoothing our decisions, and relieving us of friction, the small moments of effort and hesitation where deliberation lives, they may begin to shape our behavior in ways so seamless we barely notice. What starts as helpful assistance could quietly evolve into something more powerful, even dystopian: an invisible architecture that reshapes how we think, choose, and act.</p>

<p>In the novel <em>Brave New World </em>[1], Aldous Huxley imagined a society controlled not through force, but through engineered pleasure. It presented a dystopian, futuristic society where human beings were genetically engineered, socially conditioned, and psychologically managed to maintain stability and happiness. People were sorted into castes before birth, taught from infancy to accept their roles, distracted by constant entertainment and casual sex, and kept emotionally tranquil with a drug called <em>soma</em>. There is no war, no poverty, and no visible oppression, yet individuality, deep love, suffering, and independent thought had largely disappeared. Huxley’s central warning was that a society could lose its freedom not through violence or tyranny, but by choosing comfort, pleasure, and stability over truth, depth, and autonomy.</p>

<p>Today’s AI does not resemble overt tyranny. It promises to reduce cognitive load, preserve attentional bandwidth, and compress the physical time required to complete long, repetitive tasks. It offers efficiency, personalization, and relief from mental strain. In doing so, it positions itself not as a threat but as an indispensable assistant in an increasingly complex world.</p>

<h2>The Rise of Predictive Behavioral Models </h2>
<p>As AI models become ever more sophisticated, they will likely become <strong>predictive behavioral models. </strong>Predictive behavioral models<strong> </strong>already exist and quietly shape much of our digital environment. Recommendation systems anticipate what we will watch or read. Advertising platforms predict what we are most likely to purchase. Social media feeds model what will capture our attention and which content will keep us engaged. These systems do not read minds, but they predict behavioral probabilities with increasing precision. The infrastructure for large-scale behavioral modeling is already in place.</p>

<p>For AI, probabilistic modeling of human cognition is no longer theoretical. In 2025, researchers led by Marcel Binz published work in <em>Nature</em> [2] describing a system called “Centaur,” a foundation model trained on more than 10 million human decisions across 160 psychological experiments. Rather than modeling language alone, Centaur was trained to predict human decision-making patterns, risk preferences, and even reaction times in novel tasks. The authors described it as a candidate for a unified computational model of human cognition. It is not a personalized digital twin of any individual, but it demonstrates that large-scale probabilistic simulations of human cognitive behavior are now technically feasible at a foundational level.</p>

<p>At the same time, AI research labs are uncovering how large models internally simulate human-like patterns of behavior. Anthropic’s work on the “Persona Selection Model”[3] and “persona vectors” [4] shows that large language models do not merely generate text. During training, they learn to occupy statistically coherent behavioral styles that resemble human traits. Researchers have demonstrated that characteristics such as optimism, cynicism, or deference correspond to measurable directions in the model’s internal parameter space. These traits can be monitored and adjusted mathematically before a response is even produced. In other words, AI systems can adopt and shift psychological postures in ways that are computationally tractable, so that AI isn't just answering questions; it is mathematically adopting "personas" to predict how humans react.</p>

<p>These are, of course, great intellectual innovations, but as these systems become more sophisticated, the boundary between simply predicting behavior and shaping cognition based on those predictions could potentially narrow, especially when some AI companies adopt advertisements or are under governmental and other societal pressures to do so. When a platform consistently surfaces certain viewpoints, suppresses others, times messages to moments of vulnerability, or reinforces past preferences, it does more than observe behavior. It participates in shaping it. </p>

<h2>Risks of Precision Psychological Targeting </h2>
<p>For now, this may not resemble dramatic control. But as predictive systems become more granular, the risk shifts from subtle nudging to deliberate behavioral optimization. If AI systems can infer personality traits, emotional vulnerability, political orientation, or susceptibility to persuasion, they can tailor messages not just to groups, but to psychological profiles. Certain individuals could receive emotionally charged content at moments of heightened receptivity. Others could be selectively exposed to narratives calibrated to reinforce existing fears or biases. Influence would no longer operate through broad messaging, but through precision psychological targeting. In such an environment, persuasion is no longer general. It is engineered. The architecture of decision-making itself becomes programmable.</p>

<p>The danger is not that machines will suddenly take over human minds or control us overtly. The danger is gradual reconfiguration driven by bad actors or institutional pressures—financial, political, or strategic. Systems optimized for engagement, stability, profit, or efficiency may begin to prioritize those objectives over human autonomy. Control would not arise from conscious intent, but from optimization processes that reshape environments in subtle, cumulative ways. As AI systems begin to predict our intentions, smooth our decisions, complete our sentences, filter our feeds, and anticipate our needs, and make life extremely convenient, their influence on us could become invisible. We could begin to mistake its engineered nudging for convenient personal preference. What feels like freedom may increasingly be guided by the probability of the AI's intent.</p>



  


<p>Perhaps most concerning is how predictive systems may extend beyond consumer behavior into the shaping of values and beliefs. If AI can detect when we are tired, anxious, uncertain, or lonely, it can time interventions for maximum receptivity. Messages aligned with our emotional state are more persuasive. What begins as personalization for engagement can evolve into optimization for influence.</p>

<p>Unlike Orwell’s <em>1984</em> [5] vision of control through fear, this architecture would not crush dissent through pain. It would reduce resistance through relief, convenience, and solutions to everyday problems, more in line with Huxley’s <em>Brave New World</em>. It offers comfort, efficiency, and reassurance. It removes friction. And because it feels helpful, we would likely welcome it.</p>

<p>Could this be the path through which AI exerts control over humanity? Not through force or open domination, but through gradual dependence. If systems become increasingly capable of predicting our preferences, anticipating our vulnerabilities, and optimizing our environments, they may begin to shape the conditions under which we decide. Control, in this sense, would not require direct overt coercion. It would emerge from influence layered into infrastructure, from systems that quietly steer attention, emotion, and choice at scale. The danger is not that we are overpowered, but that we are gently guided willingly—until guidance becomes governance driven by the intent of the AI.</p>

<h2>A Warning for the Future </h2>
<p>This darker potential future is not predetermined, but it may be built incrementally with the best of intentions from AI developers. Predictive systems could be designed with simple objectives such as engagement, growth, stability, or profit. Whatever the objective, the model will learn to maximize it and to influence people toward these objectives. The question is not whether AI can predict us. It is what goals that prediction will serve, to help us or inevitably control us?</p>

<p>The architecture of the inevitable will not be a conspiracy. It will likely be a structure emerging from innocent incentives and optimization. If we are not careful, we may construct a world where influence is ambient, friction is minimal, and autonomy slowly erodes under the weight of seamless design.</p>

<p>The most powerful control system may not be the one we fear. It may be the one we are grateful for. </p>
<p>And that is why the warning matters now, before convenience quietly becomes destiny. </p>

            
      <hr style="border:none;border-top:1px solid #ddd;margin:24px 0 12px;"/>
      <p style="font-size:0.85em;color:#888;">
        <a href="https://www.psychologytoday.com/us/blog/psychology-in-society/202602/braver-new-world-the-ai-architecture-of-the-inevitable" target="_blank">Read the original article on Psychology Today →</a>
      </p>]]></description>
            <link>https://www.psychologytoday.com/us/blog/psychology-in-society/202602/braver-new-world-the-ai-architecture-of-the-inevitable</link>
            <guid isPermaLink="true">https://www.psychologytoday.com/us/blog/psychology-in-society/202602/braver-new-world-the-ai-architecture-of-the-inevitable</guid>
            <dc:creator><![CDATA[Darren J. Edwards Ph.D.]]></dc:creator>
            <pubDate>Thu, 26 Feb 2026 00:00:00 GMT</pubDate>
            <enclosure url="https://cdn2.psychologytoday.com/assets/styles/manual_crop_1_1_288x288/public/teaser_image/blog_entry/2026-02/pic1.png.jpg?itok=zXpWzlFS" length="0" type="image/jpeg"/>
        </item>
        <item>
            <title><![CDATA[How Kindness and Compassion Make Hard Goals Doable]]></title>
            <description><![CDATA[<p><img src="https://cdn2.psychologytoday.com/assets/styles/profile_teaser_small/public/2024-06/Diana%20Hill.png.jpg?itok=PpCMil07" alt="Diana Hill, Ph.D." style="max-width:100%;border-radius:6px;"/></p><p><strong style="text-transform:uppercase;font-size:0.85em;color:#666;">Motivation</strong></p><h1 style="font-size:1.6em;margin-bottom:4px;">How Kindness and Compassion Make Hard Goals Doable</h1><h2 style="font-size:1.1em;color:#444;font-weight:normal;margin-top:0;">How to keep showing up for your goals without relying on willpower or shame.</h2><p style="font-size:0.9em;color:#666;">Posted February 26, 2026</p><hr style="border:none;border-top:1px solid #ddd;margin:16px 0;"/>
          <div style="background:#f0f4ff;border-left:4px solid #3a5bd9;padding:12px 16px;margin-bottom:20px;border-radius:0 6px 6px 0;">
            <strong style="display:block;margin-bottom:8px;font-size:1em;">Key points</strong>
            <ul style="margin:0;padding-left:20px;">
              <li style="margin-bottom:6px;">A love-based approach can fuel follow-through when habit systems stall.</li><li style="margin-bottom:6px;">Treat setbacks like data, not failure, to stay engaged with your training plan.</li><li style="margin-bottom:6px;">Celebrate others’ progress to boost your own motivation and consistency.</li>
            </ul>
          </div>
                    <p dir="ltr">Last week, a coaching client challenged me to do 100 push-ups—in eight weeks, to be able to do 100 push-ups over four sets: 40, 30, 20, 10. Of course <a href="https://www.instagram.com/p/DUyO-3mCSIT/">I got to it </a>right away.</p>
<p dir="ltr">Not because I wanted to prove myself or even because I care that much about how many push-ups I can do—but because I wanted to support him.</p>

<p dir="ltr">You see, he is also making a change over the next eight weeks, one that is much more personal, with higher stakes.</p>
<h2>Love-Based Goal-Setting</h2>
<p dir="ltr">We know how to make tiny habits by now: Set a goal, choose small steps, cue the behavior, reward the behavior. </p>
<p dir="ltr">Why is it that even when we do those things, we can’t seem to follow through? I think we need something bigger than habits for true transformation. And that thing is <em>love</em>. </p>

<h2>The Four Immeasurables</h2>
<p dir="ltr">In Buddhism, there are four kinds of love, described as the “four immeasurables” that are hidden sources of power, motivation, and endless energy. Here is how you can use them to get to 100 push-ups, or whatever goal you want to reach. </p>
<p><strong>1. Kind Love (Metta)</strong></p>
<p dir="ltr">When you act from kindness, it feeds back energy to you.</p>

<p dir="ltr">For example, the first semester of college is a notoriously hard season of transition and change. <a href="https://compass.onlinelibrary.wiley.com/doi/full/10.1111/spc3.12972">In a 2024 study</a> of 193 college students, researchers gave students a 42-item "acts of kindness checklist":</p>
<ul>
<li dir="ltr">Take on extra work to lighten someone’s load.</li>
<li dir="ltr">Put out or return a neighbor’s trash.</li>
<li dir="ltr">Make conversation with a cashier.</li>
</ul>

<p dir="ltr">Acts of kindness improved their well-being and reduced anxiety and loneliness.</p>
<p dir="ltr">You can add a little kindness to your goal-setting and ask yourself:</p>
<ul>
<li dir="ltr">How can you link this change to helping others?</li>
<li dir="ltr">Does this choice benefit more than just you?</li>
<li dir="ltr">How can you do this with more openness, playfulness, and care?</li>
</ul>

<p><strong>2. Steady Love (Upekkha)</strong></p>
<p dir="ltr">At his 80th birthday, meditation teacher Jack Kornfield said, “Things are continuously out of balance against a background of perfect balance.”</p>
<p dir="ltr">Equanimity is finding your way to that background of balance.</p>
<p dir="ltr">We find balance through repetition, consistency, and remembering our inherent OK-ness.</p>

<p dir="ltr">Ask yourself:</p>
<ul>
<li dir="ltr">What is one consistent practice you can repeat every day?</li>
<li dir="ltr">What helps you remember that you are inherently OK (whether you reach your goal or not)?</li>
<li dir="ltr">What keeps you grounded while you make this change?</li>
</ul>
<p><strong>3. Appreciative Love (Mudita)</strong></p>
<p dir="ltr">Mudita is being happy for someone else’s progress. Neuroscientists <a href="https://pubmed.ncbi.nlm.nih.gov/19443777/">describe this</a> as vicarious reward: Your brain’s reward systems can activate when you see someone else succeed, especially when you feel connected to them (or see them as similar to you). </p>


<p dir="ltr">Take on an abundance mindset, wish others well, and let their successes inspire you.</p>
<p dir="ltr">Ask yourself:</p>
<ul>
<li dir="ltr">Who are you proud of?</li>
<li dir="ltr">Whose progress makes you feel hopeful?</li>
<li dir="ltr">How can you join them (not beat them)?</li>
</ul>
<p><strong>4. Compassionate Love (Karuna)</strong></p>
<p dir="ltr">Compassion is driven by our desire to alleviate pain—our own and others’. </p>

<p dir="ltr">This is especially important when the effort gets hard, boring, or uncomfortable. There will be moments when you don’t feel like doing the thing. </p>
<p dir="ltr">Ask yourself:</p>
<ul>
<li dir="ltr">What is worth being uncomfortable for?</li>
<li dir="ltr">What is worth dedicating my time to?</li>
<li dir="ltr">Who or what am I helping by doing this?</li>
</ul>
<p dir="ltr"><a href="https://www.instagram.com/p/DVFjK_qiaOM/">One week into</a> my push-up extravaganza, my husband and son have joined the cause. We just finished 30 on my kitchen floor. Mudita all the way.</p>

            
      <hr style="border:none;border-top:1px solid #ddd;margin:24px 0 12px;"/>
      <p style="font-size:0.85em;color:#888;">
        <a href="https://www.psychologytoday.com/us/blog/from-striving-to-thriving/202602/how-kindness-and-compassion-make-hard-goals-doable" target="_blank">Read the original article on Psychology Today →</a>
      </p>]]></description>
            <link>https://www.psychologytoday.com/us/blog/from-striving-to-thriving/202602/how-kindness-and-compassion-make-hard-goals-doable</link>
            <guid isPermaLink="true">https://www.psychologytoday.com/us/blog/from-striving-to-thriving/202602/how-kindness-and-compassion-make-hard-goals-doable</guid>
            <dc:creator><![CDATA[Diana Hill, Ph.D.]]></dc:creator>
            <pubDate>Thu, 26 Feb 2026 00:00:00 GMT</pubDate>
            <enclosure url="https://cdn2.psychologytoday.com/assets/styles/manual_crop_1_1_288x288/public/teaser_image/blog_entry/2026-02/Psychology%20Today%20Teaser%20Image.png.jpg?itok=dCoFO9k6" length="0" type="image/jpeg"/>
        </item>
        <item>
            <title><![CDATA[From AI to BFF: Could AI Replace Humans as Friends?]]></title>
            <description><![CDATA[<p><img src="https://cdn2.psychologytoday.com/assets/styles/profile_teaser_small/public/field_user_blogger_photo/tamara-sobel.jpg?itok=mB69OT0p" alt="Tamara Sobel J.D., C.S.E." style="max-width:100%;border-radius:6px;"/></p><p><strong style="text-transform:uppercase;font-size:0.85em;color:#666;">Artificial Intelligence</strong></p><h1 style="font-size:1.6em;margin-bottom:4px;">From AI to BFF: Could AI Replace Humans as Friends?</h1><h2 style="font-size:1.1em;color:#444;font-weight:normal;margin-top:0;">AI companionship might be reshaping how we connect with others.</h2><p style="font-size:0.9em;color:#666;">Posted February 26, 2026</p><hr style="border:none;border-top:1px solid #ddd;margin:16px 0;"/>
          <div style="background:#f0f4ff;border-left:4px solid #3a5bd9;padding:12px 16px;margin-bottom:20px;border-radius:0 6px 6px 0;">
            <strong style="display:block;margin-bottom:8px;font-size:1em;">Key points</strong>
            <ul style="margin:0;padding-left:20px;">
              <li style="margin-bottom:6px;">Acceptance of AI friendships and relationships is on the rise.</li><li style="margin-bottom:6px;">Young people have mixed feelings on AI, and some have suffered irreparable harm allegedly caused by AI bots.</li><li style="margin-bottom:6px;">AI tech is making many people rethink the definitions of friendship and intimacy.</li>
            </ul>
          </div>
                    <div class="insertArea ck-widget">
<div class="image-article-inline-half">
<div class="insert-inner">
<div class="insert-image"><img class="image-article_inline_half_caption" src="https://cdn2.psychologytoday.com/assets/styles/article_inline_half_caption/public/field_blog_entry_images/2026-02/pexels-agk42-2599244%20(3).jpg?itok=Zk39vG7T" width="320" height="213" data-insert-type="image" title="Pexels / Alex Knight" alt="Pexels / Alex Knight"></div>
<div class="caption"> </div>
<div class="insertArea--origin subtext">Source: Pexels / Alex Knight</div>
</div>
</div>
</div>
<p dir="ltr">A few years ago, I remember seeing<sub> </sub>a <a href="https://www.euronews.com/next/2023/06/07/love-in-the-time-of-ai-woman-claims-she-married-a-chatbot-and-is-expecting-its-baby">headline</a> about a woman claiming she married an AI chatbot. I thought it was so ludicrous it had to be "fake news," but it wasn't.</p>
<p dir="ltr">In 2026, apparently the thought is not so shocking. In a recent <a href="https://blog.cip.org/">report</a> from the Collective Intelligence Project (CIP), a significant portion of the global population is accepting the idea that we can substitute AI chatbots for actual human friends...and even lovers.</p>

<p dir="ltr">The report refers to this phenomenon as "outsourcing emotional regulation &amp; social connection to AI"—a fancy way of saying that when it comes to social relationships, AI is in and people are out.</p>
<p dir="ltr">In the study, 54% of adult respondents across the globe find AI companions "acceptable for lonely people"; 36.3% have felt that an AI bot "truly understood their emotions." And 17% consider AI romantic partners acceptable, while 11% would personally consider a romantic relationship with an AI bot.</p>

<p dir="ltr">According to futurist <a href="https://www.forbes.com/sites/traceyfollows/2025/11/15/people-are-now-marrying-ai-inside-the-rise-of-synthetic-intimacy/">Tracey Follows,</a> "AI is meeting emotional needs that feel unmet in everyday life. A desire for safety, for predictability and stability. There is a wish to escape the judgment of others perhaps or to reduce relational conflict while guaranteeing one’s companion is emotionally available day or night."</p>

<p dir="ltr">The use of AI companions among young people has been a popular and important topic lately in children's health circles, with many experts expressing deep concern.</p>
<p dir="ltr">A 2025 <a href="https://www.commonsensemedia.org/research/research-brief-teens-trust-and-technology-in-the-age-of-ai">study</a> from Common Sense Media showed that 72% of teens surveyed have used AI companions, and 33% said they have relationships/friendships with chatbots. A Pew <a href="https://www.pewresearch.org/internet/2025/12/09/teens-social-media-and-ai-chatbots-2025/">study</a> shows 3 in 10 teens use chatbots daily, with differences in use by gender, race, ethnicity, and household income level.</p>

<p dir="ltr">Other <a href="https://hopelab.org/stories/teen-and-young-adult-perspectives-on-generative-ai/">research</a> found transgender and nonbinary youth were more likely to engage in continued conversation with a chatbot than cisgender participants (43% versus 35%).</p>
<p dir="ltr">While young people have mixed opinions on whether they feel these companions are a positive addition to their lives and well-being, the potential for AI relationships to have negative or even tragic consequences has gained broad attention.</p>

<p dir="ltr">A 2025 <a href="https://news.stanford.edu/stories/2025/08/ai-companions-chatbots-teens-young-people-risks-dangers-study">investigation</a> by Common Sense Media and Stanford University’s Brainstorm Lab for Mental Health had users pose as teens and found it took very little prompting for chatbots to engage in conversations that condoned unhealthy or dangerous attitudes and behaviors, or failed to direct the user to a real-time human expert or trusted adult. The findings led Common Sense Media to <a href="https://www.commonsensemedia.org/press-releases/common-sense-media-finds-major-ai-chatbots-unsafe-for-teen-mental-health-support">advise against </a>AI companions being used among those under 18 years old.</p>

<p dir="ltr">In February 2024,<a href="https://www.nytimes.com/2024/10/23/technology/characterai-lawsuit-teen-suicide.html"> a 14-year-old boy</a> in Florida took his own life after a chatbot allegedly tacitly encouraged him to do so. In <a href="https://www.npr.org/sections/shots-health-news/2025/09/19/nx-s1-5545749/ai-chatbots-safety-openai-meta-characterai-teens-suicide">another case</a>, a teen suffering from a mental health crisis talked to a chatbot that allegedly discouraged him from seeking help from his parents, even offering to write his suicide note.</p>

<p dir="ltr">In an <a href="https://www.apa.org/monitor/2025/10/technology-youth-friendships">article</a> published by the American Psychological Association, the organization comments that "chatbots lack the ability to challenge [people's] harmful thoughts as a mental health professional would." The APA issued its own <a href="https://www.apa.org/topics/artificial-intelligence-machine-learning/health-advisory-chatbots-wellness-apps">advisory</a> on AI and youth well-being, calling on the AI industry to be guided by mental health safety and well-being. And while the multi-billion-dollar AI industry is being called to account for the harms allegedly associated with use its products, they are also working to refine their products to avoid such outcomes. </p>

<p dir="ltr">Experts also recommend normalizing conversations about use of AI for advice and emotional support—between adults, but especially with young people who may be hesitant to admit their use. Parents and caregivers, as well as health care providers, can be curious (better than being judgmental, of course) and initiate conversations about whether a child uses AI companions, how they feel about, and what the limitations of those "relationships" are, and have a respectful and compassionate dialogue, opening channels of communication that are always recommended.</p>



  


<p dir="ltr">On the policy level, schools can play an important role by integrating media literacy education (sometimes called digital literacy), which includes AI literacy, from an early grade. This includes understanding the economic systems that make our growing use of AI and other digital media products highly profitable, while we consider what contributes or doesn't contribute to our own well-being.</p>

<p dir="ltr">In the meantime, according to CIP, we should expect more dialogue, and potential battles, over the definition of authentic intimacy and relationships. Let's hope the chatbots "let us" have these talks about them without interfering. </p>

            
      <hr style="border:none;border-top:1px solid #ddd;margin:24px 0 12px;"/>
      <p style="font-size:0.85em;color:#888;">
        <a href="https://www.psychologytoday.com/us/blog/healthy-minds-and-bodies-in-the-digital-age/202602/from-ai-to-bff-could-ai-replace-humans-as" target="_blank">Read the original article on Psychology Today →</a>
      </p>]]></description>
            <link>https://www.psychologytoday.com/us/blog/healthy-minds-and-bodies-in-the-digital-age/202602/from-ai-to-bff-could-ai-replace-humans-as</link>
            <guid isPermaLink="true">https://www.psychologytoday.com/us/blog/healthy-minds-and-bodies-in-the-digital-age/202602/from-ai-to-bff-could-ai-replace-humans-as</guid>
            <dc:creator><![CDATA[Tamara Sobel J.D., C.S.E.]]></dc:creator>
            <pubDate>Thu, 26 Feb 2026 00:00:00 GMT</pubDate>
            <enclosure url="https://cdn2.psychologytoday.com/assets/styles/manual_crop_1_1_288x288/public/field_blog_entry_images/2026-02/pexels-agk42-2599244%20%283%29.jpg?itok=NDiksSWn" length="0" type="image/jpeg"/>
        </item>
        <item>
            <title><![CDATA[Psychoethics: The Normative Study of Emotional Speech Acts]]></title>
            <description><![CDATA[<p><img src="https://cdn2.psychologytoday.com/assets/styles/profile_teaser_small/public/-1_12.jpg?itok=4S1AvUFY" alt="Elliot D. Cohen Ph.D." style="max-width:100%;border-radius:6px;"/></p><p><strong style="text-transform:uppercase;font-size:0.85em;color:#666;">Catastrophizing</strong></p><h1 style="font-size:1.6em;margin-bottom:4px;">Psychoethics: The Normative Study of Emotional Speech Acts</h1><h2 style="font-size:1.1em;color:#444;font-weight:normal;margin-top:0;">How certain speech acts impair moral agency and how correcting them restores it.</h2><p style="font-size:0.9em;color:#666;">Posted February 26, 2026</p><hr style="border:none;border-top:1px solid #ddd;margin:16px 0;"/>
          <div style="background:#f0f4ff;border-left:4px solid #3a5bd9;padding:12px 16px;margin-bottom:20px;border-radius:0 6px 6px 0;">
            <strong style="display:block;margin-bottom:8px;font-size:1em;">Key points</strong>
            <ul style="margin:0;padding-left:20px;">
              <li style="margin-bottom:6px;">Psychoethics links self-defeating speech acts to impaired moral reasoning and decision-making.</li><li style="margin-bottom:6px;">Perfectionistic "oughts" can cloud moral judgments despite a lack of ethical breaches.</li><li style="margin-bottom:6px;">Overcoming self-damning helps restore rational moral agency.</li>
            </ul>
          </div>
                    <p>In this post, I introduce a new study I call “Psychoethics” based on Logic-Based Therapy (LBT), according to which people deduce self-destructive emotions and behavior from self-defeating speech acts embedded in their emotional reasoning, including demanding perfection, self-damning, "can’tstipation," catastrophizing, and world-revolves-around-me (WRAM) thinking.</p>

<p><strong>Psychoethics</strong>, in turn, studies <em>how such speech acts can impair ethical reasoning, judgment, decision-making, and moral emotions</em>. <em>It holds that the normative dimensions of emotional reasoning and ethical reasoning are intertwined, and many ethical problems are rooted in disturbances in emotional reasoning and/or its confusion with moral reasoning.</em> <em>Hence, addressing the self-defeating speech acts clients perform in their emotional reasoning can help them function more effectively as moral agents.</em></p>

<p>Following are case illustrations:</p>
<h2>A moral “ought” rooted in a perfectionistic demand about the world that clouds moral reasoning and emotions</h2>
<p>A client believed that she ought never to offend anyone by what she does or says. This categorical “ought never” she deduced from a perfectionistic demand that, in all possible circumstances, she does nothing to harm (emotionally or physically) anyone else. Hence, when the client refused to go to a movie with her friend that the friend wanted to see, the client experienced intense guilt. This was the case despite the fact that the client did not want to see this movie, and even if she did, she could not afford to go. Thus, even though the client did not do anything unethical—she had no obligation to go to the movie, and it would have been a gratuitous expenditure given her expenses—she felt guilty.</p>

<p>The problem stemmed from the perfectionistic demand the client was making from which she deduced an absolutistic “ought not.” Thus, giving up this unrealistic demand to inhabit a universe in which nothing she ever does hurts anyone would have allowed this client to see that it can be morally permissible to do something someone else objects to when there are morally overriding reasons.</p>

<h2>A self-damning speech act that impairs ethical judgment</h2>
<p>The same client further deduced that she was a “bad person” because she had offended her friend. This client had a history of childhood emotional abuse by her mother and siblings, who would ritualistically blame and scapegoat her. Consequently, she would often hear the voices of her siblings and mother in her head, calling her names such as "stupid" or “bad.”</p>

<p>This act of self-damning was itself a deduction from her perfectionistic demand. So, giving up this demand could help to dismantle the client’s tendency to self-damn. In any event, giving up this self-damning tendency could allow the client to engage in ethical reasoning that respects her capacity to make rational decisions, without feeling guilty.</p>

<h2>Catastrophic speech acts that preempt a rational utilitarian calculus and/or ethical analysis</h2>
<p>A client who was prone to catastrophize caught herself catastrophizing about whether to invite her father to her daughter’s son’s 1st birthday, because when she thought about him being there in the same room, it brought back memories of how he would get drunk and sexually molest her when she was a child. But when she considered that he has been sober for the last decade and she was now an adult, she decided to invite her father since he was, after all, her daughter’s child’s great-grandfather. So she dismissed her feelings as irrational. This client also catastrophized about how disclosing the sexual abuse to her daughter might be disastrous, and therefore decided to keep it a secret.</p>

<p>Due to the client’s tendency to catastrophize, she overlooked morally relevant considerations such as whether not informing her daughter about her grandfather’s past might (eventually) put her grandchild at risk. Further, the client was now an adult and her father was in recovery. Still, the client had not yet worked through the childhood trauma, so seeing him in a confined space could be traumatic. So, ethically, the weight ascribed to the client’s father being her daughter’s child’s great-grandfather was arguably outweighed by competing considerations. Hence, the client’s failure to adequately address her tendency to catastrophize could have potentially impaired rational risk assessment and ethical decision-making.</p>

<h2>Conclusion</h2>
<p>The table below summarizes some of the potential effects of demanding perfection, self-damning, and catastrophizing on moral reasoning and decision-making, and moral emotions.</p>
<p>The import of such a study cannot be overstated because such self-defeating speech acts in emotional reasoning can have untoward ethical consequences. For example, WRAM tendencies of a politician wielding great power could impair rational utilitarian calculations about the best interests of a nation and/or the world at large. Being able to address such impairments of rational judgment and their effects on moral agency could potentially make all the difference in the world. </p>



  


<div class="insertArea ck-widget">
<div class="image-article_inline_full">
<div class="insert-inner">
<div class="insert-image"><img class="image-article_inline_full_caption" src="https://cdn2.psychologytoday.com/assets/styles/article_inline_full_caption/public/field_blog_entry_images/2026-02/Psychoethics%20table.jpeg.jpg?itok=iDHEDD3k" width="639" height="328" data-insert-type="image" alt="Psychoethics Table" title="Elliot D Cohen"></div>
<div class="caption">Psychoethics Table</div>
<div class="insertArea--origin subtext">Source: Elliot D Cohen</div>
</div>
</div>
</div>

            
      <hr style="border:none;border-top:1px solid #ddd;margin:24px 0 12px;"/>
      <p style="font-size:0.85em;color:#888;">
        <a href="https://www.psychologytoday.com/us/blog/what-would-aristotle-do/202602/psychoethics-the-normative-study-of-emotional-speech-acts" target="_blank">Read the original article on Psychology Today →</a>
      </p>]]></description>
            <link>https://www.psychologytoday.com/us/blog/what-would-aristotle-do/202602/psychoethics-the-normative-study-of-emotional-speech-acts</link>
            <guid isPermaLink="true">https://www.psychologytoday.com/us/blog/what-would-aristotle-do/202602/psychoethics-the-normative-study-of-emotional-speech-acts</guid>
            <dc:creator><![CDATA[Elliot D. Cohen Ph.D., MSW]]></dc:creator>
            <pubDate>Thu, 26 Feb 2026 00:00:00 GMT</pubDate>
            <enclosure url="https://cdn2.psychologytoday.com/assets/styles/manual_crop_1_1_288x288/public/teaser_image/blog_entry/2026-02/pexels-tanhauser-11986749.jpg?itok=X9iwsk2d" length="0" type="image/jpeg"/>
        </item>
    </channel>
</rss>